{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import regex as re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to\n",
      "[nltk_data]     /Users/reneehall/nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Wiederaufnahme der S...' -> 'Resumption of the se...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-de-en.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-de-en.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = comtrans.aligned_sents('alignment-de-en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = [sent.words for sent in als]\n",
    "en = [sent.mots for sent in als]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, let's just select 10000 entries to reduce training time\n",
    "de = de[:10000]\n",
    "en = en[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im',\n",
       " 'Parlament',\n",
       " 'besteht',\n",
       " 'der',\n",
       " 'Wunsch',\n",
       " 'nach',\n",
       " 'einer',\n",
       " 'Aussprache',\n",
       " 'im',\n",
       " 'Verlauf',\n",
       " 'dieser',\n",
       " 'Sitzungsperiode',\n",
       " 'in',\n",
       " 'den',\n",
       " 'n√§chsten',\n",
       " 'Tagen',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'have',\n",
       " 'requested',\n",
       " 'a',\n",
       " 'debate',\n",
       " 'on',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'in',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'the',\n",
       " 'next',\n",
       " 'few',\n",
       " 'days',\n",
       " ',',\n",
       " 'during',\n",
       " 'this',\n",
       " 'part-session',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(docs):\n",
    "    processed_features = []\n",
    "    for sentence in docs:\n",
    "        # Remove all the special characters\n",
    "        words = [word for word in sentence if word.isalpha()]\n",
    "    \n",
    "        # Substituting multiple spaces with single space\n",
    "        words = [re.sub(r\"\\s{2,}\",\" \", word) for word in words]\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        words = [word.lower() for word in words]\n",
    "    \n",
    "        new_sent = \" \".join(words)\n",
    "        processed_features.append(new_sent)\n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = clean(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = clean(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiederaufnahme der sitzungsperiode'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = en\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "for i in de:\n",
    "    output = i + ' <eos>'\n",
    "    output_input = '<sos> ' + i\n",
    "    output_sentences.append(output)\n",
    "    output_sentences_inputs.append(output_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das parlament erhebt sich zu einer schweigeminute <eos>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> das parlament erhebt sich zu einer schweigeminute'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences_inputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188812 English words.\n",
      "8992 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \"of\" \"to\" \"and\" \"in\" \"is\" \"a\" \"that\" \"this\" \"we\"\n",
      "\n",
      "171280 German words.\n",
      "15880 unique German words.\n",
      "10 Most common words in the German dataset:\n",
      "\"die\" \"der\" \"und\" \"in\" \"wir\" \"zu\" \"ich\" \"das\" \"den\" \"ist\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in en for word in sentence.split()])\n",
    "de_words_counter = collections.Counter([word for sentence in de for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in en for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} German words.'.format(len([word for sentence in de for word in sentence.split()])))\n",
    "print('{} unique German words.'.format(len(de_words_counter)))\n",
    "print('10 Most common words in the German dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*de_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 8992\n",
      "Length of longest sentence in input: 39\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 15882\n",
      "Length of longest sentence in the output: 39\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (10000, 39)\n",
      "encoder_input_sequences[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 2422    2    1 1296]\n"
     ]
    }
   ],
   "source": [
    "# use 0's for padding, so that all sequences are the same length\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[0]:\", encoder_input_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (10000, 39)\n",
      "decoder_input_sequences[172]: [   4 2983    2 1488    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output_sequences.shape: (10000, 39)\n",
      "decoder_output_sequences[172]: [2983    2 1488    3    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_output_sequences.shape:\", decoder_output_sequences.shape)\n",
    "print(\"decoder_output_sequences[172]:\", decoder_output_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "# use GloVe\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'glove/glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word2idx_inputs)+1\n",
    "embedding_size = 100\n",
    "embedding_matrix = zeros((num_words, embedding_size))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8450e-01  5.1461e-01  6.5342e-01 -4.2173e-01 -8.1430e-01  5.0029e-02\n",
      " -4.1870e-01 -4.1949e-02  4.7558e-01 -5.4651e-01  4.3974e-01  2.6532e-01\n",
      "  2.1381e-01 -7.1729e-02 -1.7475e-01 -1.8682e-01 -1.2933e-01  4.7129e-01\n",
      " -6.2407e-01  5.4606e-01 -4.2295e-02 -1.1002e-01 -3.1637e-01 -6.2179e-01\n",
      " -2.4532e-02  2.5281e-01 -1.8242e-02 -8.5596e-01  9.6847e-02  3.9929e-02\n",
      " -2.7546e-02  6.8141e-01  1.8839e-01  1.2421e-02 -1.8829e-01  3.3089e-01\n",
      " -2.0723e-02  2.8868e-01  5.7478e-01 -3.4546e-01 -6.1522e-01 -1.1323e-01\n",
      "  9.6484e-02 -4.9250e-01 -6.4248e-01 -2.6363e-02  3.2317e-01 -3.1298e-01\n",
      " -4.2312e-01 -9.6755e-01  4.7128e-01  1.7033e-01  1.7940e-01  6.7091e-01\n",
      " -1.6210e-01 -1.9417e+00  2.3473e-01  1.3247e-01  1.2179e+00  6.0500e-01\n",
      " -2.5138e-01  1.0135e+00 -1.2321e-01 -2.3109e-01  1.1575e+00  3.2664e-01\n",
      "  6.7967e-01  6.2483e-01 -4.3403e-01  2.4201e-01  2.2149e-01 -1.1082e-01\n",
      " -2.3777e-01 -2.1992e-01  4.7086e-02  2.5577e-01  3.3871e-01 -3.9868e-01\n",
      " -6.4011e-01  1.9467e-01  3.8811e-01 -3.4189e-01 -6.5507e-01 -3.9797e-01\n",
      " -1.6680e+00 -6.2716e-01 -2.7891e-01 -1.4456e-04 -7.2774e-01 -3.2837e-01\n",
      "  1.7467e-01 -3.1239e-01  7.9336e-02 -3.3328e-01 -7.2548e-01 -9.4346e-02\n",
      " -4.1198e-01 -2.6415e-01  6.6953e-02  5.2008e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary['why'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                           embedding_size,\n",
    "                           weights = [embedding_matrix],\n",
    "                        input_length = max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final output shape: (number of inputs, length of the output sentence, the number of words in the output)\n",
    "# create empty output array\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 39, 15883)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it dense\n",
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encoder\n",
    "LSTM_NODES = 256\n",
    "\n",
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decoder\n",
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dense layer\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 553s 30s/step - loss: 7.0698 - accuracy: 0.4382 - val_loss: 3.5515 - val_accuracy: 0.5347\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 541s 30s/step - loss: 3.4745 - accuracy: 0.5336 - val_loss: 3.3942 - val_accuracy: 0.5488\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 534s 30s/step - loss: 3.3177 - accuracy: 0.5467 - val_loss: 3.2742 - val_accuracy: 0.5521\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 425s 24s/step - loss: 3.1686 - accuracy: 0.5550 - val_loss: 3.2336 - val_accuracy: 0.5637\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 430s 24s/step - loss: 3.0959 - accuracy: 0.5688 - val_loss: 3.1709 - val_accuracy: 0.5728\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 436s 24s/step - loss: 3.0297 - accuracy: 0.5762 - val_loss: 3.1430 - val_accuracy: 0.5765\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 493s 28s/step - loss: 2.9973 - accuracy: 0.5778 - val_loss: 3.1233 - val_accuracy: 0.5802\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 526s 29s/step - loss: 2.9451 - accuracy: 0.5829 - val_loss: 3.0968 - val_accuracy: 0.5857\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 558s 31s/step - loss: 2.8970 - accuracy: 0.5869 - val_loss: 3.0674 - val_accuracy: 0.5885\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 551s 31s/step - loss: 2.8791 - accuracy: 0.5858 - val_loss: 3.0505 - val_accuracy: 0.5894\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 526s 29s/step - loss: 2.8020 - accuracy: 0.5933 - val_loss: 3.0384 - val_accuracy: 0.5917\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 516s 29s/step - loss: 2.7803 - accuracy: 0.5937 - val_loss: 3.0215 - val_accuracy: 0.5927\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 508s 29s/step - loss: 2.7656 - accuracy: 0.5930 - val_loss: 3.0059 - val_accuracy: 0.5943\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 463s 26s/step - loss: 2.7245 - accuracy: 0.5968 - val_loss: 2.9948 - val_accuracy: 0.5962\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 538s 30s/step - loss: 2.6812 - accuracy: 0.6011 - val_loss: 2.9760 - val_accuracy: 0.5976\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 460s 25s/step - loss: 2.6749 - accuracy: 0.5994 - val_loss: 2.9659 - val_accuracy: 0.5981\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 467s 26s/step - loss: 2.6293 - accuracy: 0.6032 - val_loss: 2.9468 - val_accuracy: 0.6002\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 498s 28s/step - loss: 2.5984 - accuracy: 0.6046 - val_loss: 2.9389 - val_accuracy: 0.6018\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 537s 30s/step - loss: 2.5679 - accuracy: 0.6066 - val_loss: 2.9276 - val_accuracy: 0.6027\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 503s 28s/step - loss: 2.5519 - accuracy: 0.6064 - val_loss: 2.9213 - val_accuracy: 0.6045\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "BATCH_SIZE = 500\n",
    "EPOCHS = 20\n",
    "\n",
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: but as you will also be aware we are waiting for a commission communication on coherence together with the communication on development policy\n",
      "Response: wir m√ºssen wir nicht nicht die der union und die kommission und die kommission in der europ√§ischen union\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tests(docs):    \n",
    "    # Substituting multiple spaces with single space\n",
    "    words = [re.sub(r\"\\s{2,}\",\" \", word) for word in docs]\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    new_sent = \" \".join(words)\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_translate(text):\n",
    "    text = clean_tests(text)\n",
    "    T_text = input_tokenizer.texts_to_sequences([text])\n",
    "    pad_text = pad_sequences(T_text, maxlen=max_input_len)\n",
    "    return pad_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = ['This is a project presentation on NLP']\n",
    "test = to_translate(phrase)\n",
    "translation = translate_sentence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das ist die kommission'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2 = [sent.words for sent in als]\n",
    "en2 = [sent.mots for sent in als]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2 = clean(en2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i cannot therefore tell you whether we agreed or not on a specific action carried out by nato but it was part of an overall plan with which we do agree'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2[11030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2 = clean(de2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deshalb kann ich ihnen nicht sagen ob wir mit einer konkreten einverstanden waren oder nicht sie geh√∂ren zu einem gesamtplan mit dem wir einverstanden sind'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2[11030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
